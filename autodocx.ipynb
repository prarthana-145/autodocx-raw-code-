{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a423c95d",
   "metadata": {},
   "source": [
    "# Fine-Tuning LayoutLMv3 for Receipt Data Extraction\n",
    "\n",
    "This notebook demonstrates a complete pipeline for fine-tuning the `microsoft/layoutlmv3-base` model on the CORD dataset for token classification. The goal is to extract structured information (like menu items, prices, and totals) from receipt images.\n",
    "\n",
    "The process involves:\n",
    "1.  **Setup & Configuration**: Installing dependencies and defining entity labels.\n",
    "2.  **Data Preparation**: Loading and preprocessing the CORD dataset (images and JSON annotations).\n",
    "3.  **Model Training**: Fine-tuning the LayoutLMv3 model on the prepared dataset.\n",
    "4.  **Hyperparameter Tuning (Optional)**: Using Optuna to find the best training parameters.\n",
    "5.  **Final Training & Evaluation**: Training the model with the optimal hyperparameters.\n",
    "6.  **Inference**: Using the fine-tuned model to extract entities from a new, unseen receipt image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7bf82",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we install and import all the necessary libraries. This includes `transformers`, `datasets`, `torch`, and `pytesseract` for OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract transformers torch Pillow datasets seqeval optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d31aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 11:04:52.522000 18908 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# Basic imports for OS, data handling, and image processing\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import joblib\n",
    "import optuna\n",
    "from PIL import Image\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Hugging Face libraries for datasets and models\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    LayoutLMv3Processor, \n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Metrics for evaluation\n",
    "from seqeval.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026fd40",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "In this section, we define the schema for our receipt data, load the CORD dataset, and preprocess it into a format suitable for training LayoutLMv3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf8f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2.1: Define Label Schema\n",
    "# This defines all possible entity categories from the CORD dataset.\n",
    "categories = [\n",
    "    \"menu.cnt\", \"menu.discountprice\", \"menu.etc\", \"menu.itemsubtotal\", \"menu.nm\",\n",
    "    \"menu.num\", \"menu.price\", \"menu.sub_cnt\", \"menu.sub_etc\", \"menu.sub_nm\",\n",
    "    \"menu.sub_price\", \"menu.sub_unitprice\", \"menu.unitprice\", \"menu.vatyn\",\n",
    "    \"sub_total.discount_price\", \"sub_total.etc\", \"sub_total.othersvc_price\",\n",
    "    \"sub_total.service_price\", \"sub_total.subtotal_price\", \"sub_total.tax_price\",\n",
    "    \"total.cashprice\", \"total.changeprice\", \"total.creditcardprice\",\n",
    "    \"total.emoneyprice\", \"total.menuqty_cnt\", \"total.menutype_cnt\",\n",
    "    \"total.total_etc\", \"total.total_price\", \"void_menu.nm\", \"void_menu.price\"\n",
    "]\n",
    "\n",
    "# Build BIO (Beginning, Inside, Outside) tag schema and create label-to-ID mappings.\n",
    "label_list = [f\"B-{c}\" for c in categories] + [f\"I-{c}\" for c in categories]\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "num_labels = len(label_list) # Store the total number of labels for the model\n",
    "\n",
    "# Step 2.2: Initialize the LayoutLMv3 Processor\n",
    "# The processor handles tokenization, image processing, and alignment of text with layout data.\n",
    "# `apply_ocr=False` is crucial because we are providing our own OCR'd text and boxes from the dataset.\n",
    "processor = LayoutLMv3Processor.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    apply_ocr=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d86ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: Define Functions to Parse and Load the CORD Dataset\n",
    "\n",
    "def parse_cord_file(image_path: str, json_path: str):\n",
    "    \"\"\"Parses a single image and its corresponding JSON annotation file.\"\"\"\n",
    "    data = json.load(open(json_path, encoding=\"utf-8\"))\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = img.size\n",
    "\n",
    "    words = []\n",
    "    boxes = []\n",
    "    tags = []\n",
    "\n",
    "    # Iterate over each annotated line in the JSON file\n",
    "    for line in data.get(\"valid_line\", []):\n",
    "        cat = line.get(\"category\", \"\")\n",
    "        for idx, word_info in enumerate(line.get(\"words\", [])):\n",
    "            text = word_info.get(\"text\", \"\")\n",
    "            quad = word_info.get(\"quad\", {})\n",
    "            # Calculate bounding box coordinates [xmin, ymin, xmax, ymax]\n",
    "            xmin = min(quad.get(\"x1\", 0), quad.get(\"x4\", 0))\n",
    "            ymin = min(quad.get(\"y1\", 0), quad.get(\"y2\", 0))\n",
    "            xmax = max(quad.get(\"x2\", 0), quad.get(\"x3\", 0))\n",
    "            ymax = max(quad.get(\"y3\", 0), quad.get(\"y4\", 0))\n",
    "\n",
    "            # Normalize bounding box coordinates to a 0-1000 scale\n",
    "            norm_box = [\n",
    "                int(1000 * xmin / width),\n",
    "                int(1000 * ymin / height),\n",
    "                int(1000 * xmax / width),\n",
    "                int(1000 * ymax / height),\n",
    "            ]\n",
    "            norm_box = [min(max(x, 0), 1000) for x in norm_box]\n",
    "\n",
    "            words.append(text)\n",
    "            boxes.append(norm_box)\n",
    "\n",
    "            # Assign BIO tags: 'B-' for the first word in an entity, 'I-' for subsequent words\n",
    "            prefix = \"B-\" if idx == 0 else \"I-\"\n",
    "            label = f\"{prefix}{cat}\"\n",
    "            tags.append(label2id.get(label, -100))  # Use -100 to ignore unknown labels during training\n",
    "\n",
    "    # Use the processor to tokenize words and align everything\n",
    "    encoding = processor(\n",
    "        images=img,\n",
    "        text=words,\n",
    "        boxes=boxes,\n",
    "        word_labels=tags,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {k: v.squeeze() for k, v in encoding.items()} # Remove the batch dimension\n",
    "\n",
    "def build_dataset(split_dir: str) -> Dataset:\n",
    "    \"\"\"Builds a Hugging Face Dataset from a directory split (e.g., 'train', 'dev').\"\"\"\n",
    "    examples = []\n",
    "    img_dir = os.path.join(split_dir, \"image\")\n",
    "    json_dir = os.path.join(split_dir, \"json\")\n",
    "\n",
    "    for fname in os.listdir(json_dir):\n",
    "        if not fname.lower().endswith(\".json\"): continue\n",
    "        json_path = os.path.join(json_dir, fname)\n",
    "        base = os.path.splitext(fname)[0]\n",
    "\n",
    "        # The dataset contains both .png and .jpg files, so we check for both\n",
    "        image_path = None\n",
    "        for ext in (\".png\", \".jpg\"):\n",
    "            candidate = os.path.join(img_dir, base + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                image_path = candidate\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            print(f\"⚠️ Skipping {fname} — image not found.\")\n",
    "            continue\n",
    "\n",
    "        examples.append({\"image_path\": image_path, \"json_path\": json_path})\n",
    "\n",
    "    return Dataset.from_list(examples)\n",
    "\n",
    "def load_cord_dataset(base_path: str) -> DatasetDict:\n",
    "    \"\"\"Combines train, validation, and test splits into a single DatasetDict.\"\"\"\n",
    "    return DatasetDict({\n",
    "        \"train\": build_dataset(os.path.join(base_path, \"train\")),\n",
    "        \"validation\": build_dataset(os.path.join(base_path, \"dev\")),\n",
    "        \"test\": build_dataset(os.path.join(base_path, \"test\")),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999fa564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea79cb9822f4197919861a8fe9d5091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cf2217f0ab4550a5b0c46c6f7c81b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7641bbf83d8c4106b12ed5d50d7998d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded and encoded.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'bbox', 'labels', 'pixel_values'],\n",
      "        num_rows: 651\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'bbox', 'labels', 'pixel_values'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'bbox', 'labels', 'pixel_values'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 2.4: Load and Process the Dataset\n",
    "# NOTE: This cell failed previously due to a FileNotFoundError. \n",
    "# Ensure the path points to the correct root directory of the CORD dataset.\n",
    "\n",
    "def preprocess(example):\n",
    "    # This helper function calls our main parsing function for each example\n",
    "    return parse_cord_file(example[\"image_path\"], example[\"json_path\"])\n",
    "\n",
    "# Replace this with the actual path to your CORD dataset folder\n",
    "dataset_path = \"C:\\\\Users\\\\praag\\\\Desktop\\\\scrapiq\\\\CORD-20250714T102918Z-1-001\\\\CORD\"\n",
    "\n",
    "try:\n",
    "    # Load the raw dataset (paths to images and jsons)\n",
    "    raw_dataset = load_cord_dataset(dataset_path)\n",
    "\n",
    "    # Apply the preprocessing function to tokenize and encode the entire dataset\n",
    "    encoded_dataset = raw_dataset.map(\n",
    "        preprocess, \n",
    "        remove_columns=[\"image_path\", \"json_path\"]\n",
    "    )\n",
    "    print(\"Dataset successfully loaded and encoded.\")\n",
    "    print(encoded_dataset)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"Please verify that the `dataset_path` variable is set correctly.\")\n",
    "    # Create a placeholder to avoid subsequent NameErrors\n",
    "    encoded_dataset = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead55024",
   "metadata": {},
   "source": [
    "## 3. Model Training (Baseline)\n",
    "\n",
    "With the data prepared, we can now define our model, set up training arguments, and run an initial training loop to establish a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e07ad2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 3.1: Initialize the Model\n",
    "# We load the pre-trained LayoutLMv3 model and configure it for our specific token classification task.\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    num_labels=num_labels, # The number of labels we defined earlier\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07fdd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Define Training Arguments\n",
    "# These arguments control various aspects of the training process, like batch size, learning rate, and saving strategy.\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"logs\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,        \n",
    "    metric_for_best_model=\"eval_f1\", # Use F1-score to determine the best model checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e23eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: Define Evaluation Metrics\n",
    "# This function calculates metrics like F1-score and a detailed classification report during evaluation.\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Align predictions and labels, ignoring the -100 tokens\n",
    "    true_labels = []\n",
    "    true_preds  = []\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        valid_idxs = label_seq != -100\n",
    "        true_labels.append([id2label[l] for l in label_seq[valid_idxs]])\n",
    "        true_preds.append ([id2label[p] for p, v in zip(pred_seq, valid_idxs) if v])\n",
    "\n",
    "    # Calculate F1 score and the full classification report\n",
    "    f1 = f1_score(true_labels, true_preds)\n",
    "    report = classification_report(true_labels, true_preds, output_dict=True, zero_division=\"0\")\n",
    "\n",
    "    metrics = {\"f1\": f1}\n",
    "    # Flatten the report to be compatible with the Trainer logs\n",
    "    for main_key, sub_dict in report.items():\n",
    "        if isinstance(sub_dict, dict):\n",
    "            for sub_key, value in sub_dict.items():\n",
    "                metrics[f\"{main_key}_{sub_key}\"] = value\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b435f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.4: Define Custom Data Collator\n",
    "# This function is responsible for creating batches of data. It pads sequences to the same length.\n",
    "def collate_fn(examples):\n",
    "    # Stack the pixel_values (image data) into a single tensor\n",
    "    pixel_values = torch.stack([torch.tensor(e[\"pixel_values\"]) for e in examples])\n",
    "    \n",
    "    # Collect other features into lists\n",
    "    input_ids      = [e[\"input_ids\"]      for e in examples]\n",
    "    attention_mask = [e[\"attention_mask\"] for e in examples]\n",
    "    bbox           = [e[\"bbox\"]           for e in examples]\n",
    "    labels         = [e[\"labels\"]         for e in examples]\n",
    "    \n",
    "    # Use the tokenizer's pad method to handle padding for text-based features\n",
    "    batch_tokens = processor.tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": input_ids, \"attention_mask\": attention_mask,\n",
    "            \"bbox\": bbox, \"labels\": labels\n",
    "        },\n",
    "        padding=\"longest\", return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Merge the pixel_values back into the final batch dictionary\n",
    "    batch = {\"pixel_values\": pixel_values, **batch_tokens}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1281449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a LayoutLMv3TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='6520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/6520 00:42 < 15:17:06, 0.12 it/s, Epoch 0.02/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      7\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      8\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,             \n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping training because the dataset could not be loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1625\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1626\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1627\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1628\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1629\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:1966\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 3.5: Initialize and Run the Trainer\n",
    "# This cell previously failed because `encoded_dataset` was not defined.\n",
    "# It should now run correctly if the data preparation step was successful.\n",
    "\n",
    "if encoded_dataset is not None:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=encoded_dataset[\"train\"],       \n",
    "        eval_dataset=encoded_dataset[\"validation\"], \n",
    "        tokenizer=processor.tokenizer, # Pass the tokenizer for padding\n",
    "        data_collator=collate_fn,                    \n",
    "        compute_metrics=compute_metrics,             \n",
    "    )\n",
    "\n",
    "    # Start the training process\n",
    "    trainer.train()\n",
    "else:\n",
    "    print(\"Skipping training because the dataset could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7524858",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with Optuna (Optional)\n",
    "\n",
    "This section demonstrates how to use Optuna to search for the best hyperparameters (like learning rate, batch size, and number of epochs) to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a54308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Define the Objective Function for Optuna\n",
    "\n",
    "def model_init():\n",
    "    \"\"\"Initializes a new model for each trial to ensure no weight leakage.\"\"\"\n",
    "    return LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\",\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"This function defines one Optuna trial, which trains and evaluates a model with a given set of hyperparameters.\"\"\"\n",
    "    # Suggest hyperparameters for the trial\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"outputs/optuna/{trial.number}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\", # No need to save checkpoints during HPO\n",
    "        learning_rate=trial.suggest_float(\"lr\", 1e-5, 1e-4, log=True),\n",
    "        per_device_train_batch_size=trial.suggest_categorical(\"batch\", [2, 4]),\n",
    "        num_train_epochs=trial.suggest_int(\"epochs\", 3, 8),\n",
    "        logging_dir=f\"logs/optuna/{trial.number}\",\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init, # A function that returns a new model\n",
    "        args=args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        tokenizer=processor.tokenizer,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    return metrics[\"eval_f1\"]  # The objective is to maximize the F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b824bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Run the Hyperparameter Search\n",
    "# This cell was previously interrupted. Re-running it will start a new search.\n",
    "\n",
    "if encoded_dataset is not None:\n",
    "    # Create a study. 'direction=\"maximize\"' means Optuna will try to find params that maximize the objective's return value (F1 score).\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    # Start the optimization process for a set number of trials\n",
    "    study.optimize(objective, n_trials=3, timeout=7200) # Reduced to 3 trials for a quick example\n",
    "    \n",
    "    # Save the study results for later use\n",
    "    joblib.dump(study, \"optuna_study.pkl\")\n",
    "    \n",
    "    print(\"--- Optuna Search Complete ---\")\n",
    "    print(\"Best F1 score:\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n",
    "else:\n",
    "    print(\"Skipping hyperparameter tuning because the dataset could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679cec14",
   "metadata": {},
   "source": [
    "## 5. Final Model Training\n",
    "\n",
    "After finding the optimal hyperparameters, we retrain the model one last time on the full training set using these best parameters and save the final artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18382265-1977-4ba8-87b5-8e43fc37f471",
   "metadata": {},
   "source": [
    "### Load Best Hyperparameters\n",
    "This cell loads the best parameters found by the Optuna study and defines the final training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeb7e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best parameters from Optuna study: {'lr': 2.2431387959905245e-05, 'batch': 4, 'epochs': 8}\n"
     ]
    }
   ],
   "source": [
    "# Step 5.1: Load the Best Hyperparameters and Define Final Training Arguments\n",
    "\n",
    "try:\n",
    "    # Load the results from the saved Optuna study\n",
    "    study = joblib.load(\"optuna_study.pkl\")\n",
    "    best_params = study.best_params\n",
    "    print(f\"Loaded best parameters from Optuna study: {best_params}\")\n",
    "\n",
    "    # Create the final training arguments using the best hyperparams\n",
    "    best_args = TrainingArguments(\n",
    "        output_dir=\"final_model_outputs\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=best_params['lr'],\n",
    "        per_device_train_batch_size=best_params['batch'],\n",
    "        per_device_eval_batch_size=best_params['batch'], # Use the same batch size for eval\n",
    "        num_train_epochs=best_params['epochs'],\n",
    "        logging_dir=\"final_model_logs\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        save_total_limit=1, # Only keep the very best checkpoint\n",
    "        report_to=[],\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find 'optuna_study.pkl'. Using baseline training arguments instead.\")\n",
    "    best_args = args # Fallback to the initial arguments if Optuna wasn't run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c0b57-050a-458f-ac8c-c3a744300542",
   "metadata": {},
   "source": [
    "### Run Final Training\n",
    "This cell initializes a new model and trains it using the best hyperparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f8fa1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/1304 00:43 < 5:12:11, 0.07 it/s, Epoch 0.02/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 23\u001b[0m\n\u001b[0;32m     12\u001b[0m final_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mfinal_model,\n\u001b[0;32m     14\u001b[0m     args\u001b[38;5;241m=\u001b[39mbest_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train the final model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m final_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[0;32m     26\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m final_trainer\u001b[38;5;241m.\u001b[39mevaluate(encoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1625\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1626\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1627\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1628\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1629\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:1966\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 5.2: Initialize and Run the Final Trainer\n",
    "\n",
    "# Initialize a fresh model to train from scratch with the best parameters\n",
    "final_model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "if encoded_dataset is not None:\n",
    "    final_trainer = Trainer(\n",
    "        model=final_model,\n",
    "        args=best_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        tokenizer=processor.tokenizer,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the final model\n",
    "    final_trainer.train()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_metrics = final_trainer.evaluate(encoded_dataset[\"test\"])\n",
    "    print(\"--- Final Test Set Evaluation ---\")\n",
    "    print(test_metrics)\n",
    "\n",
    "    # Save the final model and processor\n",
    "    final_model_path = \"final_layoutlmv3_model\"\n",
    "    final_trainer.save_model(final_model_path)\n",
    "    processor.save_pretrained(final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "else:\n",
    "    print(\"Skipping final training because the dataset could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84e81e",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "This section shows how to use the final, fine-tuned model to extract information from a new receipt image that was not in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3ecc3-fd18-4dae-8326-75733dfc36aa",
   "metadata": {},
   "source": [
    "### Load Fine-Tuned Model for Inference\n",
    "This cell loads the final model and processor that were saved after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e84ec950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from final_layoutlmv3_model\n"
     ]
    }
   ],
   "source": [
    "# Step 6.1: Load the Fine-Tuned Model and Processor\n",
    "\n",
    "final_model_path = \"final_layoutlmv3_model\"\n",
    "\n",
    "try:\n",
    "    inference_processor = LayoutLMv3Processor.from_pretrained(final_model_path, apply_ocr=False)\n",
    "    inference_model = LayoutLMv3ForTokenClassification.from_pretrained(final_model_path)\n",
    "    inference_model.eval() # Set the model to evaluation mode\n",
    "    print(f\"Successfully loaded model from {final_model_path}\")\n",
    "except OSError:\n",
    "    print(f\"Error: Model not found at {final_model_path}. Please run the final training step first.\")\n",
    "    inference_processor = None\n",
    "    inference_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2e5eb-c388-4a54-82d4-66042b382af9",
   "metadata": {},
   "source": [
    "### Define the Inference Pipeline\n",
    "This function takes an image path, performs OCR, runs the model prediction, and returns a DataFrame of words and their predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8793d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RECEIPT</td>\n",
       "      <td>B-total.cashprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East</td>\n",
       "      <td>B-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Repair</td>\n",
       "      <td>I-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inc.</td>\n",
       "      <td>B-menu.price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1912</td>\n",
       "      <td>B-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>payable</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>to:</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>East</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Repair</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Inc.</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                label\n",
       "0    RECEIPT    B-total.cashprice\n",
       "1       East            B-menu.nm\n",
       "2     Repair            I-menu.nm\n",
       "3       Inc.         B-menu.price\n",
       "4       1912            B-menu.nm\n",
       "..       ...                  ...\n",
       "110  payable  I-total.changeprice\n",
       "111      to:  I-total.changeprice\n",
       "112     East  I-total.changeprice\n",
       "113   Repair  I-total.changeprice\n",
       "114     Inc.  I-total.changeprice\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6.2: Define the Inference Pipeline\n",
    "\n",
    "def infer_from_image(image_path: str):\n",
    "    \"\"\"Performs OCR and model prediction on a single image file.\"\"\"\n",
    "    if inference_model is None or inference_processor is None:\n",
    "        print(\"Inference model is not loaded. Cannot proceed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1. OCR the image to get words and boxes\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    words, boxes = [], []\n",
    "    for i, txt in enumerate(data[\"text\"]):\n",
    "        t = txt.strip()\n",
    "        if not t or not re.search(r\"\\w\", t): continue\n",
    "        x, y, wi, hi = (data[\"left\"][i], data[\"top\"][i], data[\"width\"][i], data[\"height\"][i])\n",
    "        norm_box = [\n",
    "            int(1000 * x / w), int(1000 * y / h),\n",
    "            int(1000 * (x+wi) / w), int(1000 * (y+hi) / h)\n",
    "        ]\n",
    "        boxes.append([min(max(v,0),1000) for v in norm_box])\n",
    "        words.append(t)\n",
    "\n",
    "    # 2. Encode the OCR data\n",
    "    encoding = inference_processor(\n",
    "        img, text=words, boxes=boxes, return_tensors=\"pt\", \n",
    "        truncation=True, padding=\"max_length\", max_length=512\n",
    "    )\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "    # 3. Predict with the model\n",
    "    with torch.no_grad():\n",
    "        logits = inference_model(**encoding).logits.squeeze(0)\n",
    "    preds = logits.argmax(-1).tolist()\n",
    "\n",
    "    # 4. Map token predictions back to word-level predictions\n",
    "    word_pred = {}\n",
    "    for idx, wid in enumerate(word_ids):\n",
    "        if wid is not None and wid not in word_pred:\n",
    "            word_pred[wid] = preds[idx]\n",
    "\n",
    "    # 5. Create a result DataFrame\n",
    "    rows = []\n",
    "    for i, w in enumerate(words):\n",
    "        label_id = word_pred.get(i, -100)\n",
    "        label = id2label.get(label_id, \"O\") # Use .get for safety\n",
    "        rows.append({\"word\": w, \"label\": label})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = infer_from_image(\"C:\\\\Users\\\\praag\\\\Downloads\\\\rr.png\")\n",
    "# only show predicted entities (non-O)\n",
    "display(df[df[\"label\"] != \"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af0363-493d-440e-88ff-43414dd45935",
   "metadata": {},
   "source": [
    "### Define Post-Processing Functions\n",
    "These functions take the raw model output (words and labels) and aggregate them into a more structured and readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1302e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: Define Post-Processing Functions\n",
    "\n",
    "def aggregate_entities(df):\n",
    "    \"\"\"Groups tokens into coherent entities based on their BIO labels.\"\"\"\n",
    "    df_filtered = df[df.label != \"O\"].copy()\n",
    "    if df_filtered.empty:\n",
    "        return []\n",
    "    \n",
    "    # Extract the base entity type from the BIO tag (e.g., 'B-menu.nm' -> 'menu.nm')\n",
    "    df_filtered[\"entity\"] = df_filtered[\"label\"].str.split(pat=\"-\", n=1).str[1]\n",
    "\n",
    "    # Group consecutive words of the same entity type\n",
    "    records = []\n",
    "    for _, group in df_filtered.groupby((df_filtered.entity != df_filtered.entity.shift()).cumsum()):\n",
    "        entity_type = group.entity.iloc[0]\n",
    "        text = \" \".join(group.word.tolist())\n",
    "        records.append({\"entity\": entity_type, \"text\": text})\n",
    "    \n",
    "    return records\n",
    "\n",
    "def format_receipt_output(records):\n",
    "    \"\"\"Pivots the extracted records into a human-readable table and a structured dictionary.\"\"\"\n",
    "    if not records:\n",
    "        print(\"No entities found to format.\")\n",
    "        return {}\n",
    "\n",
    "    # Build a dictionary, allowing for multiple values per entity\n",
    "    receipt_dict = defaultdict(list)\n",
    "    for rec in records:\n",
    "        receipt_dict[rec[\"entity\"]].append(rec[\"text\"])\n",
    "    \n",
    "    # For entities that only appear once, convert them from a list to a single value\n",
    "    final_dict = {k: v[0] if len(v) == 1 else v for k, v in receipt_dict.items()}\n",
    "    \n",
    "    # Display as a styled Pandas DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    pivot = (\n",
    "        df.assign(item_number=lambda d: d.groupby(\"entity\").cumcount())\n",
    "        .pivot(index=\"item_number\", columns=\"entity\", values=\"text\")\n",
    "        .fillna(\"\").reset_index(drop=True)\n",
    "    )\n",
    "    display(pivot.style.set_caption(\"🧾 Extracted Receipt Data\"))\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fc123-f7f7-427a-8559-487d2c0381cc",
   "metadata": {},
   "source": [
    "### (Optional) Aggregate and Display Raw Predictions\n",
    "This cell aggregates the raw word predictions into entities for a quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b927395-a536-42ec-8297-a8d4ffffd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total.cashprice      → RECEIPT\n",
      "menu.nm              → East Repair\n",
      "menu.price           → Inc.\n",
      "menu.nm              → 1912 Harvest Lane New York, NY\n",
      "menu.num             → 12210\n",
      "total.menuqty_cnt    → BILLTO SHIP TO RECEIPT\n",
      "total.creditcardprice → us-001\n",
      "menu.num             → John\n",
      "menu.sub_nm          → Smith\n",
      "menu.num             → John\n",
      "menu.sub_nm          → Smith\n",
      "total.menuqty_cnt    → RECEIPT\n",
      "total.changeprice    → DATE\n",
      "menu.num             → 11/02/2019\n",
      "menu.cnt             → 2\n",
      "menu.nm              → Court Square\n",
      "menu.num             → 3787\n",
      "menu.nm              → Pineview Drive\n",
      "total.menuqty_cnt    → Post\n",
      "menu.nm              → New York, NY\n",
      "menu.num             → 12210\n",
      "menu.nm              → Cambridge, MA 12210\n",
      "menu.sub_nm          → oy\n",
      "menu.num             → 2312/2019\n",
      "total.changeprice    → DUE DATE 26/02/2019\n",
      "menu.nm              → FEI II IO IOI IOI OI OO\n",
      "menu.unitprice       → IO IOI\n",
      "menu.nm              → IO IOI IOI III\n",
      "menu.sub_nm          → II\n",
      "menu.sub_price       → TOS IOS IOS IOS IK\n",
      "total.menuqty_cnt    → ay DESCRIPTION UNIT\n",
      "sub_total.tax_price  → PRICE\n",
      "menu.sub_price       → AMOUNT\n",
      "menu.cnt             → 1\n",
      "menu.nm              → Front and rear brake cables\n",
      "menu.price           → 100.00 100.00\n",
      "menu.cnt             → 2\n",
      "menu.nm              → New set of pedal arms\n",
      "menu.unitprice       → 15.00\n",
      "menu.price           → 30.00\n",
      "menu.cnt             → 3\n",
      "menu.nm              → Labor hrs\n",
      "menu.unitprice       → 5.00\n",
      "menu.price           → 15.00\n",
      "sub_total.subtotal_price → ‘Subtotal 145.00\n",
      "sub_total.tax_price  → Sales Tax 6.25% 9.08\n",
      "total.total_price    → TOTAL $154.06\n",
      "total.changeprice    → Swit\n",
      "total.menuqty_cnt    → TERMS CONDITIONS oh h\n",
      "total.changeprice    → Payment is due\n",
      "total.emoneyprice    → within 15\n",
      "total.changeprice    → days. Jou\n",
      "total.menuqty_cnt    → Please\n",
      "total.creditcardprice → make checks\n",
      "total.changeprice    → payable to: East Repair Inc.\n"
     ]
    }
   ],
   "source": [
    "records = aggregate_entities(df)\n",
    "for r in records:\n",
    "    print(f\"{r['entity']:20} → {r['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314881c1-491a-4c2d-9d14-494bed8d88e4",
   "metadata": {},
   "source": [
    "### Format and Display Final Output\n",
    "This cell formats the aggregated entities into a clean table and a structured dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6eb1573-8bf8-476e-b02d-aa023a211200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8304a\">\n",
       "  <caption>🧾 Extracted Receipt Data</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >entity</th>\n",
       "      <th id=\"T_8304a_level0_col0\" class=\"col_heading level0 col0\" >menu.cnt</th>\n",
       "      <th id=\"T_8304a_level0_col1\" class=\"col_heading level0 col1\" >menu.nm</th>\n",
       "      <th id=\"T_8304a_level0_col2\" class=\"col_heading level0 col2\" >menu.num</th>\n",
       "      <th id=\"T_8304a_level0_col3\" class=\"col_heading level0 col3\" >menu.price</th>\n",
       "      <th id=\"T_8304a_level0_col4\" class=\"col_heading level0 col4\" >menu.sub_nm</th>\n",
       "      <th id=\"T_8304a_level0_col5\" class=\"col_heading level0 col5\" >menu.sub_price</th>\n",
       "      <th id=\"T_8304a_level0_col6\" class=\"col_heading level0 col6\" >menu.unitprice</th>\n",
       "      <th id=\"T_8304a_level0_col7\" class=\"col_heading level0 col7\" >sub_total.subtotal_price</th>\n",
       "      <th id=\"T_8304a_level0_col8\" class=\"col_heading level0 col8\" >sub_total.tax_price</th>\n",
       "      <th id=\"T_8304a_level0_col9\" class=\"col_heading level0 col9\" >total.cashprice</th>\n",
       "      <th id=\"T_8304a_level0_col10\" class=\"col_heading level0 col10\" >total.changeprice</th>\n",
       "      <th id=\"T_8304a_level0_col11\" class=\"col_heading level0 col11\" >total.creditcardprice</th>\n",
       "      <th id=\"T_8304a_level0_col12\" class=\"col_heading level0 col12\" >total.emoneyprice</th>\n",
       "      <th id=\"T_8304a_level0_col13\" class=\"col_heading level0 col13\" >total.menuqty_cnt</th>\n",
       "      <th id=\"T_8304a_level0_col14\" class=\"col_heading level0 col14\" >total.total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8304a_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_8304a_row0_col1\" class=\"data row0 col1\" >East Repair</td>\n",
       "      <td id=\"T_8304a_row0_col2\" class=\"data row0 col2\" >12210</td>\n",
       "      <td id=\"T_8304a_row0_col3\" class=\"data row0 col3\" >Inc.</td>\n",
       "      <td id=\"T_8304a_row0_col4\" class=\"data row0 col4\" >Smith</td>\n",
       "      <td id=\"T_8304a_row0_col5\" class=\"data row0 col5\" >TOS IOS IOS IOS IK</td>\n",
       "      <td id=\"T_8304a_row0_col6\" class=\"data row0 col6\" >IO IOI</td>\n",
       "      <td id=\"T_8304a_row0_col7\" class=\"data row0 col7\" >‘Subtotal 145.00</td>\n",
       "      <td id=\"T_8304a_row0_col8\" class=\"data row0 col8\" >PRICE</td>\n",
       "      <td id=\"T_8304a_row0_col9\" class=\"data row0 col9\" >RECEIPT</td>\n",
       "      <td id=\"T_8304a_row0_col10\" class=\"data row0 col10\" >DATE</td>\n",
       "      <td id=\"T_8304a_row0_col11\" class=\"data row0 col11\" >us-001</td>\n",
       "      <td id=\"T_8304a_row0_col12\" class=\"data row0 col12\" >within 15</td>\n",
       "      <td id=\"T_8304a_row0_col13\" class=\"data row0 col13\" >BILLTO SHIP TO RECEIPT</td>\n",
       "      <td id=\"T_8304a_row0_col14\" class=\"data row0 col14\" >TOTAL $154.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8304a_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_8304a_row1_col1\" class=\"data row1 col1\" >1912 Harvest Lane New York, NY</td>\n",
       "      <td id=\"T_8304a_row1_col2\" class=\"data row1 col2\" >John</td>\n",
       "      <td id=\"T_8304a_row1_col3\" class=\"data row1 col3\" >100.00 100.00</td>\n",
       "      <td id=\"T_8304a_row1_col4\" class=\"data row1 col4\" >Smith</td>\n",
       "      <td id=\"T_8304a_row1_col5\" class=\"data row1 col5\" >AMOUNT</td>\n",
       "      <td id=\"T_8304a_row1_col6\" class=\"data row1 col6\" >15.00</td>\n",
       "      <td id=\"T_8304a_row1_col7\" class=\"data row1 col7\" ></td>\n",
       "      <td id=\"T_8304a_row1_col8\" class=\"data row1 col8\" >Sales Tax 6.25% 9.08</td>\n",
       "      <td id=\"T_8304a_row1_col9\" class=\"data row1 col9\" ></td>\n",
       "      <td id=\"T_8304a_row1_col10\" class=\"data row1 col10\" >DUE DATE 26/02/2019</td>\n",
       "      <td id=\"T_8304a_row1_col11\" class=\"data row1 col11\" >make checks</td>\n",
       "      <td id=\"T_8304a_row1_col12\" class=\"data row1 col12\" ></td>\n",
       "      <td id=\"T_8304a_row1_col13\" class=\"data row1 col13\" >RECEIPT</td>\n",
       "      <td id=\"T_8304a_row1_col14\" class=\"data row1 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8304a_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_8304a_row2_col1\" class=\"data row2 col1\" >Court Square</td>\n",
       "      <td id=\"T_8304a_row2_col2\" class=\"data row2 col2\" >John</td>\n",
       "      <td id=\"T_8304a_row2_col3\" class=\"data row2 col3\" >30.00</td>\n",
       "      <td id=\"T_8304a_row2_col4\" class=\"data row2 col4\" >oy</td>\n",
       "      <td id=\"T_8304a_row2_col5\" class=\"data row2 col5\" ></td>\n",
       "      <td id=\"T_8304a_row2_col6\" class=\"data row2 col6\" >5.00</td>\n",
       "      <td id=\"T_8304a_row2_col7\" class=\"data row2 col7\" ></td>\n",
       "      <td id=\"T_8304a_row2_col8\" class=\"data row2 col8\" ></td>\n",
       "      <td id=\"T_8304a_row2_col9\" class=\"data row2 col9\" ></td>\n",
       "      <td id=\"T_8304a_row2_col10\" class=\"data row2 col10\" >Swit</td>\n",
       "      <td id=\"T_8304a_row2_col11\" class=\"data row2 col11\" ></td>\n",
       "      <td id=\"T_8304a_row2_col12\" class=\"data row2 col12\" ></td>\n",
       "      <td id=\"T_8304a_row2_col13\" class=\"data row2 col13\" >Post</td>\n",
       "      <td id=\"T_8304a_row2_col14\" class=\"data row2 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8304a_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_8304a_row3_col1\" class=\"data row3 col1\" >Pineview Drive</td>\n",
       "      <td id=\"T_8304a_row3_col2\" class=\"data row3 col2\" >11/02/2019</td>\n",
       "      <td id=\"T_8304a_row3_col3\" class=\"data row3 col3\" >15.00</td>\n",
       "      <td id=\"T_8304a_row3_col4\" class=\"data row3 col4\" >II</td>\n",
       "      <td id=\"T_8304a_row3_col5\" class=\"data row3 col5\" ></td>\n",
       "      <td id=\"T_8304a_row3_col6\" class=\"data row3 col6\" ></td>\n",
       "      <td id=\"T_8304a_row3_col7\" class=\"data row3 col7\" ></td>\n",
       "      <td id=\"T_8304a_row3_col8\" class=\"data row3 col8\" ></td>\n",
       "      <td id=\"T_8304a_row3_col9\" class=\"data row3 col9\" ></td>\n",
       "      <td id=\"T_8304a_row3_col10\" class=\"data row3 col10\" >Payment is due</td>\n",
       "      <td id=\"T_8304a_row3_col11\" class=\"data row3 col11\" ></td>\n",
       "      <td id=\"T_8304a_row3_col12\" class=\"data row3 col12\" ></td>\n",
       "      <td id=\"T_8304a_row3_col13\" class=\"data row3 col13\" >ay DESCRIPTION UNIT</td>\n",
       "      <td id=\"T_8304a_row3_col14\" class=\"data row3 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8304a_row4_col0\" class=\"data row4 col0\" ></td>\n",
       "      <td id=\"T_8304a_row4_col1\" class=\"data row4 col1\" >New York, NY</td>\n",
       "      <td id=\"T_8304a_row4_col2\" class=\"data row4 col2\" >3787</td>\n",
       "      <td id=\"T_8304a_row4_col3\" class=\"data row4 col3\" ></td>\n",
       "      <td id=\"T_8304a_row4_col4\" class=\"data row4 col4\" ></td>\n",
       "      <td id=\"T_8304a_row4_col5\" class=\"data row4 col5\" ></td>\n",
       "      <td id=\"T_8304a_row4_col6\" class=\"data row4 col6\" ></td>\n",
       "      <td id=\"T_8304a_row4_col7\" class=\"data row4 col7\" ></td>\n",
       "      <td id=\"T_8304a_row4_col8\" class=\"data row4 col8\" ></td>\n",
       "      <td id=\"T_8304a_row4_col9\" class=\"data row4 col9\" ></td>\n",
       "      <td id=\"T_8304a_row4_col10\" class=\"data row4 col10\" >days. Jou</td>\n",
       "      <td id=\"T_8304a_row4_col11\" class=\"data row4 col11\" ></td>\n",
       "      <td id=\"T_8304a_row4_col12\" class=\"data row4 col12\" ></td>\n",
       "      <td id=\"T_8304a_row4_col13\" class=\"data row4 col13\" >TERMS CONDITIONS oh h</td>\n",
       "      <td id=\"T_8304a_row4_col14\" class=\"data row4 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_8304a_row5_col0\" class=\"data row5 col0\" ></td>\n",
       "      <td id=\"T_8304a_row5_col1\" class=\"data row5 col1\" >Cambridge, MA 12210</td>\n",
       "      <td id=\"T_8304a_row5_col2\" class=\"data row5 col2\" >12210</td>\n",
       "      <td id=\"T_8304a_row5_col3\" class=\"data row5 col3\" ></td>\n",
       "      <td id=\"T_8304a_row5_col4\" class=\"data row5 col4\" ></td>\n",
       "      <td id=\"T_8304a_row5_col5\" class=\"data row5 col5\" ></td>\n",
       "      <td id=\"T_8304a_row5_col6\" class=\"data row5 col6\" ></td>\n",
       "      <td id=\"T_8304a_row5_col7\" class=\"data row5 col7\" ></td>\n",
       "      <td id=\"T_8304a_row5_col8\" class=\"data row5 col8\" ></td>\n",
       "      <td id=\"T_8304a_row5_col9\" class=\"data row5 col9\" ></td>\n",
       "      <td id=\"T_8304a_row5_col10\" class=\"data row5 col10\" >payable to: East Repair Inc.</td>\n",
       "      <td id=\"T_8304a_row5_col11\" class=\"data row5 col11\" ></td>\n",
       "      <td id=\"T_8304a_row5_col12\" class=\"data row5 col12\" ></td>\n",
       "      <td id=\"T_8304a_row5_col13\" class=\"data row5 col13\" >Please</td>\n",
       "      <td id=\"T_8304a_row5_col14\" class=\"data row5 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_8304a_row6_col0\" class=\"data row6 col0\" ></td>\n",
       "      <td id=\"T_8304a_row6_col1\" class=\"data row6 col1\" >FEI II IO IOI IOI OI OO</td>\n",
       "      <td id=\"T_8304a_row6_col2\" class=\"data row6 col2\" >2312/2019</td>\n",
       "      <td id=\"T_8304a_row6_col3\" class=\"data row6 col3\" ></td>\n",
       "      <td id=\"T_8304a_row6_col4\" class=\"data row6 col4\" ></td>\n",
       "      <td id=\"T_8304a_row6_col5\" class=\"data row6 col5\" ></td>\n",
       "      <td id=\"T_8304a_row6_col6\" class=\"data row6 col6\" ></td>\n",
       "      <td id=\"T_8304a_row6_col7\" class=\"data row6 col7\" ></td>\n",
       "      <td id=\"T_8304a_row6_col8\" class=\"data row6 col8\" ></td>\n",
       "      <td id=\"T_8304a_row6_col9\" class=\"data row6 col9\" ></td>\n",
       "      <td id=\"T_8304a_row6_col10\" class=\"data row6 col10\" ></td>\n",
       "      <td id=\"T_8304a_row6_col11\" class=\"data row6 col11\" ></td>\n",
       "      <td id=\"T_8304a_row6_col12\" class=\"data row6 col12\" ></td>\n",
       "      <td id=\"T_8304a_row6_col13\" class=\"data row6 col13\" ></td>\n",
       "      <td id=\"T_8304a_row6_col14\" class=\"data row6 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_8304a_row7_col0\" class=\"data row7 col0\" ></td>\n",
       "      <td id=\"T_8304a_row7_col1\" class=\"data row7 col1\" >IO IOI IOI III</td>\n",
       "      <td id=\"T_8304a_row7_col2\" class=\"data row7 col2\" ></td>\n",
       "      <td id=\"T_8304a_row7_col3\" class=\"data row7 col3\" ></td>\n",
       "      <td id=\"T_8304a_row7_col4\" class=\"data row7 col4\" ></td>\n",
       "      <td id=\"T_8304a_row7_col5\" class=\"data row7 col5\" ></td>\n",
       "      <td id=\"T_8304a_row7_col6\" class=\"data row7 col6\" ></td>\n",
       "      <td id=\"T_8304a_row7_col7\" class=\"data row7 col7\" ></td>\n",
       "      <td id=\"T_8304a_row7_col8\" class=\"data row7 col8\" ></td>\n",
       "      <td id=\"T_8304a_row7_col9\" class=\"data row7 col9\" ></td>\n",
       "      <td id=\"T_8304a_row7_col10\" class=\"data row7 col10\" ></td>\n",
       "      <td id=\"T_8304a_row7_col11\" class=\"data row7 col11\" ></td>\n",
       "      <td id=\"T_8304a_row7_col12\" class=\"data row7 col12\" ></td>\n",
       "      <td id=\"T_8304a_row7_col13\" class=\"data row7 col13\" ></td>\n",
       "      <td id=\"T_8304a_row7_col14\" class=\"data row7 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_8304a_row8_col0\" class=\"data row8 col0\" ></td>\n",
       "      <td id=\"T_8304a_row8_col1\" class=\"data row8 col1\" >Front and rear brake cables</td>\n",
       "      <td id=\"T_8304a_row8_col2\" class=\"data row8 col2\" ></td>\n",
       "      <td id=\"T_8304a_row8_col3\" class=\"data row8 col3\" ></td>\n",
       "      <td id=\"T_8304a_row8_col4\" class=\"data row8 col4\" ></td>\n",
       "      <td id=\"T_8304a_row8_col5\" class=\"data row8 col5\" ></td>\n",
       "      <td id=\"T_8304a_row8_col6\" class=\"data row8 col6\" ></td>\n",
       "      <td id=\"T_8304a_row8_col7\" class=\"data row8 col7\" ></td>\n",
       "      <td id=\"T_8304a_row8_col8\" class=\"data row8 col8\" ></td>\n",
       "      <td id=\"T_8304a_row8_col9\" class=\"data row8 col9\" ></td>\n",
       "      <td id=\"T_8304a_row8_col10\" class=\"data row8 col10\" ></td>\n",
       "      <td id=\"T_8304a_row8_col11\" class=\"data row8 col11\" ></td>\n",
       "      <td id=\"T_8304a_row8_col12\" class=\"data row8 col12\" ></td>\n",
       "      <td id=\"T_8304a_row8_col13\" class=\"data row8 col13\" ></td>\n",
       "      <td id=\"T_8304a_row8_col14\" class=\"data row8 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_8304a_row9_col0\" class=\"data row9 col0\" ></td>\n",
       "      <td id=\"T_8304a_row9_col1\" class=\"data row9 col1\" >New set of pedal arms</td>\n",
       "      <td id=\"T_8304a_row9_col2\" class=\"data row9 col2\" ></td>\n",
       "      <td id=\"T_8304a_row9_col3\" class=\"data row9 col3\" ></td>\n",
       "      <td id=\"T_8304a_row9_col4\" class=\"data row9 col4\" ></td>\n",
       "      <td id=\"T_8304a_row9_col5\" class=\"data row9 col5\" ></td>\n",
       "      <td id=\"T_8304a_row9_col6\" class=\"data row9 col6\" ></td>\n",
       "      <td id=\"T_8304a_row9_col7\" class=\"data row9 col7\" ></td>\n",
       "      <td id=\"T_8304a_row9_col8\" class=\"data row9 col8\" ></td>\n",
       "      <td id=\"T_8304a_row9_col9\" class=\"data row9 col9\" ></td>\n",
       "      <td id=\"T_8304a_row9_col10\" class=\"data row9 col10\" ></td>\n",
       "      <td id=\"T_8304a_row9_col11\" class=\"data row9 col11\" ></td>\n",
       "      <td id=\"T_8304a_row9_col12\" class=\"data row9 col12\" ></td>\n",
       "      <td id=\"T_8304a_row9_col13\" class=\"data row9 col13\" ></td>\n",
       "      <td id=\"T_8304a_row9_col14\" class=\"data row9 col14\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8304a_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_8304a_row10_col0\" class=\"data row10 col0\" ></td>\n",
       "      <td id=\"T_8304a_row10_col1\" class=\"data row10 col1\" >Labor hrs</td>\n",
       "      <td id=\"T_8304a_row10_col2\" class=\"data row10 col2\" ></td>\n",
       "      <td id=\"T_8304a_row10_col3\" class=\"data row10 col3\" ></td>\n",
       "      <td id=\"T_8304a_row10_col4\" class=\"data row10 col4\" ></td>\n",
       "      <td id=\"T_8304a_row10_col5\" class=\"data row10 col5\" ></td>\n",
       "      <td id=\"T_8304a_row10_col6\" class=\"data row10 col6\" ></td>\n",
       "      <td id=\"T_8304a_row10_col7\" class=\"data row10 col7\" ></td>\n",
       "      <td id=\"T_8304a_row10_col8\" class=\"data row10 col8\" ></td>\n",
       "      <td id=\"T_8304a_row10_col9\" class=\"data row10 col9\" ></td>\n",
       "      <td id=\"T_8304a_row10_col10\" class=\"data row10 col10\" ></td>\n",
       "      <td id=\"T_8304a_row10_col11\" class=\"data row10 col11\" ></td>\n",
       "      <td id=\"T_8304a_row10_col12\" class=\"data row10 col12\" ></td>\n",
       "      <td id=\"T_8304a_row10_col13\" class=\"data row10 col13\" ></td>\n",
       "      <td id=\"T_8304a_row10_col14\" class=\"data row10 col14\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2df067f57d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Structured Receipt Dictionary:\n",
      "{\n",
      "  \"total.cashprice\": \"RECEIPT\",\n",
      "  \"menu.nm\": [\n",
      "    \"East Repair\",\n",
      "    \"1912 Harvest Lane New York, NY\",\n",
      "    \"Court Square\",\n",
      "    \"Pineview Drive\",\n",
      "    \"New York, NY\",\n",
      "    \"Cambridge, MA 12210\",\n",
      "    \"FEI II IO IOI IOI OI OO\",\n",
      "    \"IO IOI IOI III\",\n",
      "    \"Front and rear brake cables\",\n",
      "    \"New set of pedal arms\",\n",
      "    \"Labor hrs\"\n",
      "  ],\n",
      "  \"menu.price\": [\n",
      "    \"Inc.\",\n",
      "    \"100.00 100.00\",\n",
      "    \"30.00\",\n",
      "    \"15.00\"\n",
      "  ],\n",
      "  \"menu.num\": [\n",
      "    \"12210\",\n",
      "    \"John\",\n",
      "    \"John\",\n",
      "    \"11/02/2019\",\n",
      "    \"3787\",\n",
      "    \"12210\",\n",
      "    \"2312/2019\"\n",
      "  ],\n",
      "  \"total.menuqty_cnt\": [\n",
      "    \"BILLTO SHIP TO RECEIPT\",\n",
      "    \"RECEIPT\",\n",
      "    \"Post\",\n",
      "    \"ay DESCRIPTION UNIT\",\n",
      "    \"TERMS CONDITIONS oh h\",\n",
      "    \"Please\"\n",
      "  ],\n",
      "  \"total.creditcardprice\": [\n",
      "    \"us-001\",\n",
      "    \"make checks\"\n",
      "  ],\n",
      "  \"menu.sub_nm\": [\n",
      "    \"Smith\",\n",
      "    \"Smith\",\n",
      "    \"oy\",\n",
      "    \"II\"\n",
      "  ],\n",
      "  \"total.changeprice\": [\n",
      "    \"DATE\",\n",
      "    \"DUE DATE 26/02/2019\",\n",
      "    \"Swit\",\n",
      "    \"Payment is due\",\n",
      "    \"days. Jou\",\n",
      "    \"payable to: East Repair Inc.\"\n",
      "  ],\n",
      "  \"menu.cnt\": [\n",
      "    \"2\",\n",
      "    \"1\",\n",
      "    \"2\",\n",
      "    \"3\"\n",
      "  ],\n",
      "  \"menu.unitprice\": [\n",
      "    \"IO IOI\",\n",
      "    \"15.00\",\n",
      "    \"5.00\"\n",
      "  ],\n",
      "  \"menu.sub_price\": [\n",
      "    \"TOS IOS IOS IOS IK\",\n",
      "    \"AMOUNT\"\n",
      "  ],\n",
      "  \"sub_total.tax_price\": [\n",
      "    \"PRICE\",\n",
      "    \"Sales Tax 6.25% 9.08\"\n",
      "  ],\n",
      "  \"sub_total.subtotal_price\": \"\\u2018Subtotal 145.00\",\n",
      "  \"total.total_price\": \"TOTAL $154.06\",\n",
      "  \"total.emoneyprice\": \"within 15\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "receipt_struct = format_receipt_output(records)\n",
    "print(\"\\n📂 Structured Receipt Dictionary:\")\n",
    "import json; print(json.dumps(receipt_struct, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a2cd8d9-3505-4587-9c71-cf7352f69989",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=\"C:\\\\Users\\\\praag\\\\Downloads\\\\rr.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72593b12-9c2e-4921-9820-2c3eb01d4018",
   "metadata": {},
   "source": [
    "### (Optional) Custom OCR Line Extraction\n",
    "This function provides a more advanced way to group OCR tokens into lines, which can be useful for debugging or alternative preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a1e69d2-e05a-4b6e-8ae3-57469f76ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line  1 [96%]: RECEIPT\n",
      "Line  2 [94%]: East Repair Inc.\n",
      "Line  3 [90%]: 1912 Harvest Lane\n",
      "Line  4 [94%]: New York, NY 12210\n",
      "Line  5 [78%]: BILLTO RECEIPT # us-001\n",
      "Line  6 [90%]: John Smith John Smith RECEIPT DATE 11102/2019\n",
      "Line  7 [83%]: 2 Court Square 3787 Pineview Drive\n",
      "Line  8 [88%]: New York, NY 12210 Cambridge, MA 12210 post 2312/2019\n",
      "Line  9 [84%]: DUE DATE\n",
      "Line 10 [44%]: FIO II IOI IO IO IOI IO IOI IOI III II IOS IOS IOS IOS IK\n",
      "Line 11 [89%]: ay DESCRIPTION UNIT PRICE AMOUNT\n",
      "Line 12 [83%]: 1 Front and rear brake cables 100.00 100.00\n",
      "Line 13 [81%]: 2 New set of pedal arms 16.00 30.00\n",
      "Line 14 [71%]: 3 Labor 5.00 15.00\n",
      "Line 15 [75%]: ‘Subtotal 145.00\n",
      "Line 16 [89%]: Sales Tax 6.25% 9.08\n",
      "Line 17 [94%]: TOTAL $154.06\n",
      "Line 18 [54%]: Swit\n",
      "Line 19 [93%]: TERMS & CONDITIONS\n",
      "Line 20 [92%]: Payment is due within 15 days.\n",
      "Line 21 [82%]: oh\n",
      "Line 22 [90%]: Jou Please make checks payable to: East Repair Inc.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "def extract_lines_custom(\n",
    "    image_path, \n",
    "    y_tol=10,         # max pixel vertical deviation to stay on same line\n",
    "    conf_thresh=50,   # drop tokens < this confidence\n",
    "):\n",
    "    # 1) OCR → raw tokens\n",
    "    img = cv2.imread(\"C:\\\\Users\\\\praag\\\\Downloads\\\\rr.png\")\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "    tokens = []\n",
    "    for i, txt in enumerate(data['text']):\n",
    "        txt = txt.strip()\n",
    "        conf = int(data['conf'][i])\n",
    "        if not txt or conf < conf_thresh:\n",
    "            continue\n",
    "        x, y, w, h = (data[k][i] for k in ('left','top','width','height'))\n",
    "        cy = y + h/2\n",
    "        tokens.append({\n",
    "            'text': txt,\n",
    "            'x': x, 'y': y, 'w': w, 'h': h, 'cy': cy,\n",
    "            'conf': conf\n",
    "        })\n",
    "\n",
    "    # 2) Sort tokens top→down, then left→right\n",
    "    tokens.sort(key=lambda t: (t['cy'], t['x']))\n",
    "\n",
    "    # 3) Group into lines by y_tol\n",
    "    lines = []\n",
    "    if tokens:\n",
    "        current = {\n",
    "            'cy_sum': tokens[0]['cy'], \n",
    "            'count': 1, \n",
    "            'tokens': [tokens[0]]\n",
    "        }\n",
    "        for t in tokens[1:]:\n",
    "            # compute current running line center\n",
    "            cur_center = current['cy_sum'] / current['count']\n",
    "            if abs(t['cy'] - cur_center) <= y_tol:\n",
    "                # same line\n",
    "                current['tokens'].append(t)\n",
    "                current['cy_sum'] += t['cy']\n",
    "                current['count'] += 1\n",
    "            else:\n",
    "                # flush old line\n",
    "                lines.append(current['tokens'])\n",
    "                # start new line\n",
    "                current = {\n",
    "                    'cy_sum': t['cy'], \n",
    "                    'count': 1, \n",
    "                    'tokens': [t]\n",
    "                }\n",
    "        lines.append(current['tokens'])\n",
    "\n",
    "    # 4) Build final text+bbox per line\n",
    "    result = []\n",
    "    for idx, toks in enumerate(lines, start=1):\n",
    "        # sort left→right\n",
    "        toks.sort(key=lambda t: t['x'])\n",
    "        line_text = ' '.join(t['text'] for t in toks)\n",
    "        # bounding‑box union\n",
    "        x0 = min(t['x'] for t in toks)\n",
    "        y0 = min(t['y'] for t in toks)\n",
    "        x1 = max(t['x']+t['w'] for t in toks)\n",
    "        y1 = max(t['y']+t['h'] for t in toks)\n",
    "        result.append({\n",
    "            'line_id': idx,\n",
    "            'text': line_text,\n",
    "            'bbox': (x0, y0, x1-x0, y1-y0),\n",
    "            'avg_conf': sum(t['conf'] for t in toks)/len(toks)\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "# === example call ===\n",
    "lines = extract_lines_custom(\n",
    "    image_path,\n",
    "    y_tol=8,       # you may need to tweak this per image resolution\n",
    "    conf_thresh=30 # drop very low‑confidence tokens\n",
    ")\n",
    "\n",
    "for L in lines:\n",
    "    print(f\"Line {L['line_id']:2d} [{L['avg_conf']:.0f}%]: {L['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2608e901-a48f-46ed-aead-600351c97ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RECEIPT</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East</td>\n",
       "      <td>B-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Repair</td>\n",
       "      <td>I-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inc.</td>\n",
       "      <td>I-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1912</td>\n",
       "      <td>B-menu.cnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>payable</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>to:</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>East</td>\n",
       "      <td>B-menu.nm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Repair</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Inc.</td>\n",
       "      <td>I-total.changeprice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                label\n",
       "0    RECEIPT  I-total.changeprice\n",
       "1       East            B-menu.nm\n",
       "2     Repair            I-menu.nm\n",
       "3       Inc.            I-menu.nm\n",
       "4       1912           B-menu.cnt\n",
       "..       ...                  ...\n",
       "110  payable  I-total.changeprice\n",
       "111      to:  I-total.changeprice\n",
       "112     East            B-menu.nm\n",
       "113   Repair  I-total.changeprice\n",
       "114     Inc.  I-total.changeprice\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#v imp code for ocr \n",
    "\n",
    "\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"final_layoutlmv3_model\", apply_ocr=False)\n",
    "model     = LayoutLMv3ForTokenClassification.from_pretrained(\"final_layoutlmv3_model\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "import pytesseract, re, torch, pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def infer_from_image(image_path):\n",
    "    # 1. OCR + normalize boxes\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    words, boxes = [], []\n",
    "    for i, txt in enumerate(data[\"text\"]):\n",
    "        t = txt.strip()\n",
    "        if not t or not re.search(r\"\\w\", t): continue\n",
    "        x, y, wi, hi = (data[\"left\"][i], data[\"top\"][i],\n",
    "                        data[\"width\"][i], data[\"height\"][i])\n",
    "        norm = [int(1000 * x / w), int(1000 * y / h),\n",
    "                int(1000 * (x+wi) / w), int(1000 * (y+hi) / h)]\n",
    "        boxes.append([min(max(v,0),1000) for v in norm])\n",
    "        words.append(t)\n",
    "\n",
    "    # 2. Encode *with* word_ids tracking\n",
    "    encoding = processor(\n",
    "        img,\n",
    "        text=words,\n",
    "        boxes=boxes,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    # HuggingFace API to get the alignment of tokens → words\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "    # 3. Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"],\n",
    "            bbox=encoding[\"bbox\"],\n",
    "            token_type_ids=encoding.get(\"token_type_ids\")  # add this if your processor provides it\n",
    "        ).logits.squeeze(0)\n",
    "  # [seq_len, num_labels]\n",
    "    preds = logits.argmax(-1).tolist()               # [seq_len]\n",
    "\n",
    "    # 4. Map subword preds → word preds (first subword only)\n",
    "    word_pred: dict[int,int] = {}\n",
    "    for idx, wid in enumerate(word_ids):\n",
    "        if wid is None: continue\n",
    "        if wid not in word_pred:  # take first subtoken of each word\n",
    "            word_pred[wid] = preds[idx]\n",
    "\n",
    "    # 5. Build final table of (word, label)\n",
    "    rows = []\n",
    "    for i, w in enumerate(words):\n",
    "        lid = word_pred.get(i, -100)\n",
    "        lab = id2label[lid] if lid != -100 else \"O\"\n",
    "        rows.append({\"word\": w, \"label\": lab})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- Test it on your sample image ---\n",
    "df = infer_from_image(\"C:\\\\Users\\\\praag\\\\Downloads\\\\rr.png\")\n",
    "# only show predicted entities (non-O)\n",
    "display(df[df[\"label\"] != \"O\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7fe4ee7-7893-4e09-8289-4df79561be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = infer_from_image(\"C:\\\\users\\\\praag\\\\Desktop\\\\scrapiq\\\\CORD-20250714T102918Z-1-001\\\\CORD\\\\test\\\\image\\\\receipt_00013.png\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "921415be-8389-4865-a5e7-7bce8c3f4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line  1 [96%]: RECEIPT\n",
      "Line  2 [94%]: East Repair Inc.\n",
      "Line  3 [90%]: 1912 Harvest Lane\n",
      "Line  4 [94%]: New York, NY 12210\n",
      "Line  5 [78%]: BILLTO RECEIPT # us-001\n",
      "Line  6 [90%]: John Smith John Smith RECEIPT DATE 11102/2019\n",
      "Line  7 [83%]: 2 Court Square 3787 Pineview Drive\n",
      "Line  8 [88%]: New York, NY 12210 Cambridge, MA 12210 post 2312/2019\n",
      "Line  9 [84%]: DUE DATE\n",
      "Line 10 [44%]: FIO II IOI IO IO IOI IO IOI IOI III II IOS IOS IOS IOS IK\n",
      "Line 11 [89%]: ay DESCRIPTION UNIT PRICE AMOUNT\n",
      "Line 12 [83%]: 1 Front and rear brake cables 100.00 100.00\n",
      "Line 13 [81%]: 2 New set of pedal arms 16.00 30.00\n",
      "Line 14 [71%]: 3 Labor 5.00 15.00\n",
      "Line 15 [75%]: ‘Subtotal 145.00\n",
      "Line 16 [89%]: Sales Tax 6.25% 9.08\n",
      "Line 17 [94%]: TOTAL $154.06\n",
      "Line 18 [54%]: Swit\n",
      "Line 19 [93%]: TERMS & CONDITIONS\n",
      "Line 20 [92%]: Payment is due within 15 days.\n",
      "Line 21 [82%]: oh\n",
      "Line 22 [90%]: Jou Please make checks payable to: East Repair Inc.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "def extract_lines_custom(\n",
    "    image_path, \n",
    "    y_tol=10,         # max pixel vertical deviation to stay on same line\n",
    "    conf_thresh=50,   # drop tokens < this confidence\n",
    "):\n",
    "    # 1) OCR → raw tokens\n",
    "    img = cv2.imread(image_path)\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "    tokens = []\n",
    "    for i, txt in enumerate(data['text']):\n",
    "        txt = txt.strip()\n",
    "        conf = int(data['conf'][i])\n",
    "        if not txt or conf < conf_thresh:\n",
    "            continue\n",
    "        x, y, w, h = (data[k][i] for k in ('left','top','width','height'))\n",
    "        cy = y + h/2\n",
    "        tokens.append({\n",
    "            'text': txt,\n",
    "            'x': x, 'y': y, 'w': w, 'h': h, 'cy': cy,\n",
    "            'conf': conf\n",
    "        })\n",
    "\n",
    "    # 2) Sort tokens top→down, then left→right\n",
    "    tokens.sort(key=lambda t: (t['cy'], t['x']))\n",
    "\n",
    "    # 3) Group into lines by y_tol\n",
    "    lines = []\n",
    "    if tokens:\n",
    "        current = {\n",
    "            'cy_sum': tokens[0]['cy'], \n",
    "            'count': 1, \n",
    "            'tokens': [tokens[0]]\n",
    "        }\n",
    "        for t in tokens[1:]:\n",
    "            # compute current running line center\n",
    "            cur_center = current['cy_sum'] / current['count']\n",
    "            if abs(t['cy'] - cur_center) <= y_tol:\n",
    "                # same line\n",
    "                current['tokens'].append(t)\n",
    "                current['cy_sum'] += t['cy']\n",
    "                current['count'] += 1\n",
    "            else:\n",
    "                # flush old line\n",
    "                lines.append(current['tokens'])\n",
    "                # start new line\n",
    "                current = {\n",
    "                    'cy_sum': t['cy'], \n",
    "                    'count': 1, \n",
    "                    'tokens': [t]\n",
    "                }\n",
    "        lines.append(current['tokens'])\n",
    "\n",
    "    # 4) Build final text+bbox per line\n",
    "    result = []\n",
    "    for idx, toks in enumerate(lines, start=1):\n",
    "        # sort left→right\n",
    "        toks.sort(key=lambda t: t['x'])\n",
    "        line_text = ' '.join(t['text'] for t in toks)\n",
    "        # bounding‑box union\n",
    "        x0 = min(t['x'] for t in toks)\n",
    "        y0 = min(t['y'] for t in toks)\n",
    "        x1 = max(t['x']+t['w'] for t in toks)\n",
    "        y1 = max(t['y']+t['h'] for t in toks)\n",
    "        result.append({\n",
    "            'line_id': idx,\n",
    "            'text': line_text,\n",
    "            'bbox': (x0, y0, x1-x0, y1-y0),\n",
    "            'avg_conf': sum(t['conf'] for t in toks)/len(toks)\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "# === example call ===\n",
    "lines = extract_lines_custom(\n",
    "    image_path, \n",
    "    y_tol=8,       # you may need to tweak this per image resolution\n",
    "    conf_thresh=30 # drop very low‑confidence tokens\n",
    ")\n",
    "\n",
    "for L in lines:\n",
    "    print(f\"Line {L['line_id']:2d} [{L['avg_conf']:.0f}%]: {L['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "133e1f70-4d13-49c3-8dce-23489c183873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForTokenClassification(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=60, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "\n",
    "# Load trained model\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"final_layoutlmv3_model\")\n",
    "\n",
    "# Load corresponding processor (use apply_ocr=True or False depending on earlier choice)\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"final_layoutlmv3_model\", apply_ocr=False)\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c63563f4-d0e7-43b8-b182-b995fdc781b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praag\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Line  1 | line_score = 0.473\n",
      "Text : RECEIPT\n",
      "Words and predictions:\n",
      "  RECEIPT         → I-sub_total.tax_price  (score 0.043)\n",
      "\n",
      "Line  2 | line_score = 0.425\n",
      "Text : East Repair Inc.\n",
      "Words and predictions:\n",
      "  East            → I-sub_total.tax_price  (score 0.043)\n",
      "  Repair          → B-menu.nm             (score 0.864)\n",
      "  Inc.            → I-menu.nm             (score 0.655)\n",
      "\n",
      "Line  3 | line_score = 0.464\n",
      "Text : 1912 Harvest Lane\n",
      "Words and predictions:\n",
      "  1912            → I-sub_total.tax_price  (score 0.043)\n",
      "  Harvest         → B-menu.nm             (score 0.532)\n",
      "  Lane            → I-menu.nm             (score 0.792)\n",
      "\n",
      "Line  4 | line_score = 0.158\n",
      "Text : New York, NY 12210\n",
      "Words and predictions:\n",
      "  New             → I-sub_total.tax_price  (score 0.043)\n",
      "  York,           → B-menu.nm             (score 0.327)\n",
      "  NY              → B-menu.nm             (score 0.124)\n",
      "  12210           → I-menu.nm             (score 0.224)\n",
      "\n",
      "Line  5 | line_score = 0.190\n",
      "Text : BILLTO RECEIPT us-001\n",
      "Words and predictions:\n",
      "  BILLTO          → I-sub_total.tax_price  (score 0.043)\n",
      "  RECEIPT         → B-total.menuqty_cnt   (score 0.249)\n",
      "  us-001          → B-total.cashprice     (score 0.165)\n",
      "\n",
      "Line  6 | line_score = 0.243\n",
      "Text : John Smith John Smith RECEIPT DATE 11102/2019\n",
      "Words and predictions:\n",
      "  John            → I-sub_total.tax_price  (score 0.044)\n",
      "  Smith           → B-menu.nm             (score 0.511)\n",
      "  John            → I-menu.nm             (score 0.645)\n",
      "  Smith           → B-menu.nm             (score 0.384)\n",
      "  RECEIPT         → I-menu.nm             (score 0.815)\n",
      "  DATE            → B-total.menuqty_cnt   (score 0.222)\n",
      "  11102/2019      → B-total.menuqty_cnt   (score 0.090)\n",
      "\n",
      "Line  7 | line_score = 0.500\n",
      "Text : 2 Court Square 3787 Pineview Drive post 2312/2019\n",
      "Words and predictions:\n",
      "  2               → I-sub_total.tax_price  (score 0.042)\n",
      "  Court           → B-menu.cnt            (score 0.982)\n",
      "  Square          → B-menu.nm             (score 0.850)\n",
      "  3787            → I-menu.nm             (score 0.820)\n",
      "  Pineview        → B-menu.num            (score 0.396)\n",
      "  Drive           → B-menu.num            (score 0.281)\n",
      "  post            → B-menu.nm             (score 0.809)\n",
      "  2312/2019       → I-menu.nm             (score 0.919)\n",
      "\n",
      "Line  8 | line_score = 0.266\n",
      "Text : New York, NY 12210 Cambridge, MA 12210\n",
      "Words and predictions:\n",
      "  New             → I-sub_total.tax_price  (score 0.043)\n",
      "  York,           → B-menu.nm             (score 0.592)\n",
      "  NY              → I-menu.nm             (score 0.164)\n",
      "  12210           → B-menu.num            (score 0.207)\n",
      "  Cambridge,      → B-menu.num            (score 0.182)\n",
      "  MA              → B-menu.num            (score 0.405)\n",
      "  12210           → B-menu.num            (score 0.209)\n",
      "\n",
      "Line  9 | line_score = 0.220\n",
      "Text : DUE DATE\n",
      "Words and predictions:\n",
      "  DUE             → I-sub_total.tax_price  (score 0.043)\n",
      "  DATE            → B-total.menuqty_cnt   (score 0.374)\n",
      "\n",
      "Line 10 | line_score = 0.185\n",
      "Text : II IOS IOS IOS IOS IK\n",
      "Words and predictions:\n",
      "  II              → I-sub_total.tax_price  (score 0.043)\n",
      "  IOS             → B-menu.nm             (score 0.107)\n",
      "  IOS             → B-menu.unitprice      (score 0.163)\n",
      "  IOS             → B-menu.price          (score 0.173)\n",
      "  IOS             → B-menu.price          (score 0.290)\n",
      "  IK              → B-menu.price          (score 0.213)\n",
      "\n",
      "Line 11 | line_score = 0.250\n",
      "Text : ay DESCRIPTION UNIT PRICE AMOUNT\n",
      "Words and predictions:\n",
      "  ay              → I-sub_total.tax_price  (score 0.043)\n",
      "  DESCRIPTION     → B-total.menuqty_cnt   (score 0.767)\n",
      "  UNIT            → B-total.menuqty_cnt   (score 0.375)\n",
      "  PRICE           → I-total.menuqty_cnt   (score 0.214)\n",
      "  AMOUNT          → B-total.menuqty_cnt   (score 0.332)\n",
      "\n",
      "Line 12 | line_score = 0.798\n",
      "Text : 1 Front and rear brake cables 100.00 100.00\n",
      "Words and predictions:\n",
      "  1               → I-sub_total.tax_price  (score 0.043)\n",
      "  Front           → B-menu.cnt            (score 0.992)\n",
      "  and             → B-menu.nm             (score 0.992)\n",
      "  rear            → I-menu.nm             (score 0.994)\n",
      "  brake           → I-menu.nm             (score 0.994)\n",
      "  cables          → I-menu.nm             (score 0.994)\n",
      "  100.00          → I-menu.nm             (score 0.994)\n",
      "  100.00          → B-menu.unitprice      (score 0.910)\n",
      "\n",
      "Line 13 | line_score = 0.818\n",
      "Text : 2 set of pedal arms 16.00 30.00\n",
      "Words and predictions:\n",
      "  2               → I-sub_total.tax_price  (score 0.043)\n",
      "  set             → B-menu.cnt            (score 0.992)\n",
      "  of              → B-menu.nm             (score 0.992)\n",
      "  pedal           → I-menu.nm             (score 0.994)\n",
      "  arms            → I-menu.nm             (score 0.994)\n",
      "  16.00           → I-menu.nm             (score 0.994)\n",
      "  30.00           → B-menu.unitprice      (score 0.955)\n",
      "\n",
      "Line 14 | line_score = 0.766\n",
      "Text : 3 Labor 5.00 15.00\n",
      "Words and predictions:\n",
      "  3               → I-sub_total.tax_price  (score 0.043)\n",
      "  Labor           → B-menu.cnt            (score 0.991)\n",
      "  5.00            → B-menu.nm             (score 0.987)\n",
      "  15.00           → B-menu.unitprice      (score 0.891)\n",
      "\n",
      "Line 15 | line_score = 0.519\n",
      "Text : ‘Subtotal 145.00\n",
      "Words and predictions:\n",
      "  ‘Subtotal       → I-sub_total.tax_price  (score 0.043)\n",
      "  145.00          → B-sub_total.subtotal_price  (score 0.827)\n",
      "\n",
      "Line 16 | line_score = 0.758\n",
      "Text : Sales Tax 6.25% 9.08\n",
      "Words and predictions:\n",
      "  Sales           → I-sub_total.tax_price  (score 0.043)\n",
      "  Tax             → B-sub_total.tax_price  (score 0.823)\n",
      "  6.25%           → I-sub_total.tax_price  (score 0.899)\n",
      "  9.08            → I-sub_total.tax_price  (score 0.952)\n",
      "\n",
      "Line 17 | line_score = 0.711\n",
      "Text : TOTAL $154.06\n",
      "Words and predictions:\n",
      "  TOTAL           → I-sub_total.tax_price  (score 0.043)\n",
      "  $154.06         → B-total.total_price   (score 0.988)\n",
      "\n",
      "Line 18 | line_score = 0.376\n",
      "Text : Swit\n",
      "Words and predictions:\n",
      "  Swit            → I-sub_total.tax_price  (score 0.043)\n",
      "\n",
      "Line 19 | line_score = 0.213\n",
      "Text : TERMS & CONDITIONS\n",
      "Words and predictions:\n",
      "  TERMS           → I-sub_total.tax_price  (score 0.042)\n",
      "  &               → B-total.menuqty_cnt   (score 0.488)\n",
      "  CONDITIONS      → B-total.menuqty_cnt   (score 0.175)\n",
      "\n",
      "Line 20 | line_score = 0.171\n",
      "Text : Payment is due within 15 days.\n",
      "Words and predictions:\n",
      "  Payment         → I-sub_total.tax_price  (score 0.043)\n",
      "  is              → B-total.creditcardprice  (score 0.309)\n",
      "  due             → I-total.creditcardprice  (score 0.205)\n",
      "  within          → I-total.creditcardprice  (score 0.246)\n",
      "  15              → I-total.emoneyprice   (score 0.171)\n",
      "  days.           → I-total.emoneyprice   (score 0.168)\n",
      "\n",
      "Line 21 | line_score = 0.101\n",
      "Text : oh\n",
      "Words and predictions:\n",
      "  oh              → I-sub_total.tax_price  (score 0.042)\n",
      "\n",
      "Line 22 | line_score = 0.217\n",
      "Text : Jou Please make checks payable to: East Repair Inc.\n",
      "Words and predictions:\n",
      "  Jou             → I-sub_total.tax_price  (score 0.043)\n",
      "  Please          → B-total.menuqty_cnt   (score 0.243)\n",
      "  make            → B-total.menuqty_cnt   (score 0.236)\n",
      "  checks          → I-total.creditcardprice  (score 0.145)\n",
      "  payable         → I-total.creditcardprice  (score 0.231)\n",
      "  to:             → I-total.changeprice   (score 0.230)\n",
      "  East            → I-total.changeprice   (score 0.435)\n",
      "  Repair          → B-menu.nm             (score 0.091)\n",
      "  Inc.            → I-total.changeprice   (score 0.149)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from PIL import Image\n",
    "\n",
    "# === Load your saved model & processor ===\n",
    "model_dir = \"final_layoutlmv3_model\"\n",
    "processor = LayoutLMv3Processor.from_pretrained(model_dir, apply_ocr=False)\n",
    "model     = LayoutLMv3ForTokenClassification.from_pretrained(model_dir)\n",
    "model.eval()\n",
    "\n",
    "# === 1) Modify your extractor to keep tokens per line ===\n",
    "def extract_lines_with_tokens(image_path, y_tol=10, conf_thresh=50):\n",
    "    import cv2, pytesseract\n",
    "    img = cv2.imread(image_path)\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    tokens = []\n",
    "    for i, txt in enumerate(data['text']):\n",
    "        txt = txt.strip()\n",
    "        conf = int(data['conf'][i])\n",
    "        if not txt or conf < conf_thresh:\n",
    "            continue\n",
    "        x,y,w,h = (data[k][i] for k in ('left','top','width','height'))\n",
    "        cy = y + h/2\n",
    "        tokens.append(dict(text=txt, x=x,y=y,w=w,h=h, cy=cy, conf=conf))\n",
    "    tokens.sort(key=lambda t:(t['cy'],t['x']))\n",
    "    \n",
    "    # group into lines\n",
    "    lines, cur = [], None\n",
    "    for t in tokens:\n",
    "        if cur is None or abs(t['cy'] - (cur['cy_sum']/cur['count'])) > y_tol:\n",
    "            if cur: lines.append(cur)\n",
    "            cur = {'cy_sum':t['cy'], 'count':1, 'tokens':[t]}\n",
    "        else:\n",
    "            cur['tokens'].append(t)\n",
    "            cur['cy_sum'] += t['cy']; cur['count'] += 1\n",
    "    if cur: lines.append(cur)\n",
    "\n",
    "    # build result with tokens kept\n",
    "    out = []\n",
    "    for idx, L in enumerate(lines, start=1):\n",
    "        toks = sorted(L['tokens'], key=lambda t:t['x'])\n",
    "        text = \" \".join(t['text'] for t in toks)\n",
    "        x0 = min(t['x'] for t in toks); y0 = min(t['y'] for t in toks)\n",
    "        x1 = max(t['x']+t['w'] for t in toks); y1 = max(t['y']+t['h'] for t in toks)\n",
    "        avg_conf = sum(t['conf'] for t in toks)/len(toks)\n",
    "        out.append({\n",
    "            'line_id': idx,\n",
    "            'text': text,\n",
    "            'bbox': (x0,y0,x1-x0,y1-y0),\n",
    "            'avg_conf': avg_conf,\n",
    "            'tokens': toks  # keep per-word token info\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# === 2) Scoring function ===\n",
    "def score_lines(image_path, device=\"cpu\"):\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    lines = extract_lines_with_tokens(image_path)\n",
    "\n",
    "    results = []\n",
    "    W, H = img.size\n",
    "    for ln in lines:\n",
    "        words = [t['text'] for t in ln['tokens']]\n",
    "        boxes = [\n",
    "            [\n",
    "              int(1000 * t['x']       / W),\n",
    "              int(1000 * t['y']       / H),\n",
    "              int(1000 * (t['x']+t['w']) / W),\n",
    "              int(1000 * (t['y']+t['h']) / H),\n",
    "            ]\n",
    "            for t in ln['tokens']\n",
    "        ]\n",
    "\n",
    "        # tokenize + align\n",
    "        enc = processor(img, text=words, boxes=boxes,\n",
    "                        return_tensors=\"pt\", truncation=True)\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc).logits  # [1, seq_len, num_labels]\n",
    "        probs = F.softmax(out, dim=-1).squeeze(0)  # [seq_len, num_labels]\n",
    "        scores, preds = probs.max(dim=-1)         # both [seq_len]\n",
    "\n",
    "        # map back to tokens\n",
    "        word_preds = preds.cpu().tolist()\n",
    "        word_scores= scores.cpu().tolist()\n",
    "        line_score = float(scores.mean().cpu())\n",
    "\n",
    "        # build result\n",
    "        results.append({\n",
    "            'line_id':    ln['line_id'],\n",
    "            'text':       ln['text'],\n",
    "            'word_preds': word_preds,\n",
    "            'word_scores':word_scores,\n",
    "            'line_score': line_score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# === 3) Test & display ===\n",
    "# 4) Display lines with per‑word category & score\n",
    "res = score_lines(ip, device=\"cpu\")\n",
    "\n",
    "for r in res:\n",
    "    print(f\"\\nLine {r['line_id']:2d} | line_score = {r['line_score']:.3f}\")\n",
    "    print(\"Text :\", r['text'])\n",
    "    print(\"Words and predictions:\")\n",
    "    for tok, pid, sc in zip(\n",
    "        extract_lines_with_tokens(ip)[r['line_id']-1]['tokens'],  # original tokens\n",
    "        r['word_preds'],\n",
    "        r['word_scores']\n",
    "    ):\n",
    "        category = id2label[pid]\n",
    "        print(f\"  {tok['text']:<15} → {category:<20}  (score {sc:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3edde87e-d9a4-4d30-bdb0-5c1aa844b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip=image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ed7b4d70-059c-4ac9-9083-f819cee4565c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume you already ran:\n",
    "# res = score_lines(\"rr.png\")\n",
    "\n",
    "line_ids = [r['line_id']   for r in res]\n",
    "scores   = [r['line_score'] for r in res]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(line_ids, scores)\n",
    "plt.xlabel(\"Line Number\")\n",
    "plt.ylabel(\"Average Line Score\")\n",
    "plt.title(\"LayoutLMv3 Line Scores\")\n",
    "plt.xticks(line_ids)  \n",
    "plt.ylim(0, 1.0)       # scores are probabilities 0–1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42162fb1-e672-44fc-9b65-030d420f2646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Line 12 | line_score = 0.798\n",
      "Text : 1 Front and rear brake cables 100.00 100.00\n",
      "Words and predictions:\n",
      "  1               → I-sub_total.tax_price  (score 0.043)\n",
      "  Front           → B-menu.cnt            (score 0.992)\n",
      "  and             → B-menu.nm             (score 0.992)\n",
      "  rear            → I-menu.nm             (score 0.994)\n",
      "  brake           → I-menu.nm             (score 0.994)\n",
      "  cables          → I-menu.nm             (score 0.994)\n",
      "  100.00          → I-menu.nm             (score 0.994)\n",
      "  100.00          → B-menu.unitprice      (score 0.910)\n",
      "\n",
      "Line 13 | line_score = 0.818\n",
      "Text : 2 set of pedal arms 16.00 30.00\n",
      "Words and predictions:\n",
      "  2               → I-sub_total.tax_price  (score 0.043)\n",
      "  set             → B-menu.cnt            (score 0.992)\n",
      "  of              → B-menu.nm             (score 0.992)\n",
      "  pedal           → I-menu.nm             (score 0.994)\n",
      "  arms            → I-menu.nm             (score 0.994)\n",
      "  16.00           → I-menu.nm             (score 0.994)\n",
      "  30.00           → B-menu.unitprice      (score 0.955)\n",
      "\n",
      "Line 14 | line_score = 0.766\n",
      "Text : 3 Labor 5.00 15.00\n",
      "Words and predictions:\n",
      "  3               → I-sub_total.tax_price  (score 0.043)\n",
      "  Labor           → B-menu.cnt            (score 0.991)\n",
      "  5.00            → B-menu.nm             (score 0.987)\n",
      "  15.00           → B-menu.unitprice      (score 0.891)\n",
      "\n",
      "Line 16 | line_score = 0.758\n",
      "Text : Sales Tax 6.25% 9.08\n",
      "Words and predictions:\n",
      "  Sales           → I-sub_total.tax_price  (score 0.043)\n",
      "  Tax             → B-sub_total.tax_price  (score 0.823)\n",
      "  6.25%           → I-sub_total.tax_price  (score 0.899)\n",
      "  9.08            → I-sub_total.tax_price  (score 0.952)\n",
      "\n",
      "Line 17 | line_score = 0.711\n",
      "Text : TOTAL $154.06\n",
      "Words and predictions:\n",
      "  TOTAL           → I-sub_total.tax_price  (score 0.043)\n",
      "  $154.06         → B-total.total_price   (score 0.988)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.70\n",
    "filtered = [r for r in res if r['line_score'] > threshold]\n",
    "\n",
    "for r in filtered:\n",
    "    print(f\"\\nLine {r['line_id']:2d} | line_score = {r['line_score']:.3f}\")\n",
    "    print(\"Text :\", r['text'])\n",
    "    print(\"Words and predictions:\")\n",
    "    tokens = extract_lines_with_tokens(ip)[r['line_id']-1]['tokens']\n",
    "    for tok, pid, sc in zip(tokens, r['word_preds'], r['word_scores']):\n",
    "        category = id2label[pid]\n",
    "        print(f\"  {tok['text']:<15} → {category:<20}  (score {sc:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bcc56f07-4975-408f-b0e3-411c0ffd163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Main Lines\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>text</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>UNIT_PRICE</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1 Front and rear brake cables 100.00 100.00</td>\n",
       "      <td></td>\n",
       "      <td>Front and rear brake cables</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>2 set of pedal arms 16.00 30.00</td>\n",
       "      <td></td>\n",
       "      <td>set of pedal arms</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3 Labor 5.00 15.00</td>\n",
       "      <td></td>\n",
       "      <td>Labor</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_id                                         text QUANTITY  \\\n",
       "0       12  1 Front and rear brake cables 100.00 100.00            \n",
       "1       13              2 set of pedal arms 16.00 30.00            \n",
       "2       14                           3 Labor 5.00 15.00            \n",
       "\n",
       "                   DESCRIPTION UNIT_PRICE PRICE  \n",
       "0  Front and rear brake cables                   \n",
       "1            set of pedal arms                   \n",
       "2                        Labor                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Footer Lines\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_id</th>\n",
       "      <th>text</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>UNIT_PRICE</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>Sales Tax 6.25% 9.08</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>TOTAL $154.06</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_id                  text QUANTITY DESCRIPTION UNIT_PRICE PRICE\n",
       "0       16  Sales Tax 6.25% 9.08                                      \n",
       "1       17         TOTAL $154.06                                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from PIL import Image\n",
    "\n",
    "# ── 0) Load model & processor ──────────────────────────────────────────────\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"final_layoutlmv3_model\", apply_ocr=False)\n",
    "model     = LayoutLMv3ForTokenClassification.from_pretrained(\"final_layoutlmv3_model\").eval()\n",
    "\n",
    "# ── 1) Define footer keywords (all‐case variants) ────────────────────────────\n",
    "base_footer = [\"subtotal\", \"tax\", \"total\", \"gst\", \"discount\", \"service\", \"charge\",\"sales\",\"sales tax\",\"cash\",\"change\",\"amount\",\"sale tax\",\"price\"]\n",
    "footer_keywords = set()\n",
    "for w in base_footer:\n",
    "    footer_keywords.update({w.lower(), w.upper(), w.capitalize()})\n",
    "\n",
    "# ── 2) Define simplified category groups ────────────────────────────────────\n",
    "category_map = {\n",
    "    \"QUANTITY\":    [\"menu.cnt\", \"menu.sub_cnt\", \"total.menuqty_cnt\"],\n",
    "    \"UNIT_PRICE\":  [\"menu.unitprice\", \"menu.sub_unitprice\"],\n",
    "    \"PRICE\":       [\"menu.price\", \"menu.discountprice\",\n",
    "                    \"menu.itemsubtotal\", \"menu.sub_price\", \"void_menu.price\"],\n",
    "    # FOOTER types also treated as numeric for re‑scoring\n",
    "    \"FOOTER\":      [\"sub_total.discount_price\", \"sub_total.etc\",\n",
    "                    \"sub_total.othersvc_price\", \"sub_total.service_price\",\n",
    "                    \"sub_total.subtotal_price\", \"sub_total.tax_price\",\n",
    "                    \"total.cashprice\", \"total.changeprice\",\n",
    "                    \"total.creditcardprice\", \"total.emoneyprice\",\n",
    "                    \"total.total_etc\", \"total.total_price\"]\n",
    "}\n",
    "\n",
    "# ── 3) Helpers ───────────────────────────────────────────────────────────────\n",
    "def is_footer_word(tok):\n",
    "    return tok.lower().strip(\"%$\") in footer_keywords\n",
    "\n",
    "# get span of lines to weight footer probability\n",
    "all_ids = [r[\"line_id\"] for r in filtered]\n",
    "min_id, max_id = min(all_ids), max(all_ids)\n",
    "span = max(1, max_id - min_id)\n",
    "\n",
    "def rescore_token(tok, line_id):\n",
    "    \"\"\"\n",
    "    Re‑run the model on this single token, boost all FOOTER probs\n",
    "    proportionally to line position (lower lines → bigger boost).\n",
    "    Returns sorted label indices and their scores.\n",
    "    \"\"\"\n",
    "    img = Image.open(ip).convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "    box = [int(1000 * tok[k] / (W if k in (\"x\",\"x2\") else H))\n",
    "           for k in (\"x\",\"y\",\"x2\",\"y2\")]  # compute normalized box\n",
    "    enc = processor(img, text=[tok[\"text\"]], boxes=[box], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**{k:v for k,v in enc.items()}).logits.squeeze(0)[0]\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    # boost FOOTER-group classes\n",
    "    boost = (line_id - min_id) / span\n",
    "    for idx, lab in id2label.items():\n",
    "        if lab in category_map[\"FOOTER\"]:\n",
    "            probs[idx] *= (1 + boost)\n",
    "\n",
    "    return list(np.argsort(probs)[::-1]), probs\n",
    "\n",
    "# ── 4) Build cleaned_preds ──────────────────────────────────────────────────\n",
    "cleaned = []\n",
    "for r in filtered:\n",
    "    toks   = extract_lines_with_tokens(ip)[r[\"line_id\"]-1][\"tokens\"]\n",
    "    preds  = r[\"word_preds\"]\n",
    "    scores = r[\"word_scores\"]\n",
    "    new_lbls = []\n",
    "\n",
    "    for i, (tok, pid, sc) in enumerate(zip(toks, preds, scores)):\n",
    "        w = tok[\"text\"]\n",
    "        orig = id2label[pid]\n",
    "\n",
    "        # 4a) First use model’s original prediction\n",
    "        label = orig\n",
    "\n",
    "        # 4b) If this is an alpha token predicted as numeric/price\n",
    "        if w.isalpha() and label in sum((category_map[c] for c in [\"QUANTITY\",\"UNIT_PRICE\",\"PRICE\"]), []):\n",
    "            # but do NOT override real descriptions (check footer first)\n",
    "            if is_footer_word(w):\n",
    "                label = \"FOOTER_DESC\"\n",
    "            else:\n",
    "                # re‑score to see if it should be footed\n",
    "                sorted_ids, prob_vec = rescore_token(tok, r[\"line_id\"])\n",
    "                # if top FOOTER candidate prob > line_score → choose FOOTER\n",
    "                for cand in sorted_ids:\n",
    "                    cand_lab = id2label[cand]\n",
    "                    if cand_lab in category_map[\"FOOTER\"] and prob_vec[cand] > r[\"line_score\"]:\n",
    "                        label = \"FOOTER_DESC\"\n",
    "                        break\n",
    "                else:\n",
    "                    label = \"DESCRIPTION\"\n",
    "\n",
    "        # 4c) Pure alpha tokens not in numeric groups → description or footer\n",
    "        elif w.isalpha():\n",
    "            label = \"FOOTER_DESC\" if is_footer_word(w) else \"DESCRIPTION\"\n",
    "\n",
    "        new_lbls.append(label)\n",
    "\n",
    "    r[\"cleaned_preds\"] = new_lbls\n",
    "    cleaned.append(r)\n",
    "\n",
    "# ── 5) Display final table ──────────────────────────────────────────────────\n",
    "rows, foot = [], []\n",
    "for r in cleaned:\n",
    "    row = {\"line_id\": r[\"line_id\"], \"text\": r[\"text\"]}\n",
    "    # bucket the tokens into the four display columns\n",
    "    buckets = defaultdict(list)\n",
    "    for w,l in zip(extract_lines_with_tokens(ip)[r[\"line_id\"]-1][\"tokens\"], r[\"cleaned_preds\"]):\n",
    "        if l in (\"QUANTITY\",\"UNIT_PRICE\",\"PRICE\"):\n",
    "            buckets[l].append(w[\"text\"])\n",
    "        elif l==\"DESCRIPTION\":\n",
    "            buckets[\"DESCRIPTION\"].append(w[\"text\"])\n",
    "        else:  # FOOTER_DESC or anything else\n",
    "            buckets[\"FOOTER_DESC\"].append(w[\"text\"])\n",
    "    row.update({\n",
    "        \"QUANTITY\":    \" \".join(buckets[\"QUANTITY\"]),\n",
    "        \"DESCRIPTION\": \" \".join(buckets[\"DESCRIPTION\"]),\n",
    "        \"UNIT_PRICE\":  \" \".join(buckets[\"UNIT_PRICE\"]),\n",
    "        \"PRICE\":       \" \".join(buckets[\"PRICE\"])\n",
    "    })\n",
    "    footer_count = sum(1 for l in r[\"cleaned_preds\"] if l == \"FOOTER_DESC\")\n",
    "    total_count  = len(r[\"cleaned_preds\"])\n",
    "\n",
    "    if footer_count / total_count >= 0.5:\n",
    "        foot.append(row)\n",
    "    else:\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "df_main   = pd.DataFrame(rows)\n",
    "df_footer = pd.DataFrame(foot)\n",
    "\n",
    "print(\"🧾 Main Lines\")\n",
    "display(df_main)\n",
    "print(\"\\n📌 Footer Lines\")\n",
    "display(df_footer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "866bf10a-2c8e-43fa-b3e4-93acb9871cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_numeric(text):\n",
    "    return bool(re.match(r'^[₹$]?\\d+(\\.\\d{1,2})?$', text.strip()))\n",
    "\n",
    "def apply_template_if_description(tokens_with_labels):\n",
    "    \"\"\"\n",
    "    Input: list of dicts: [{'text': ..., 'label': ..., 'score': ...}]\n",
    "    Returns: corrected list of labels (same length)\n",
    "    \"\"\"\n",
    "    tokens = [t['text'] for t in tokens_with_labels]\n",
    "    old_labels = [t['label'] for t in tokens_with_labels]\n",
    "    \n",
    "    # Check if any word is already labeled as DESCRIPTION\n",
    "    if \"DESCRIPTION\" not in old_labels:\n",
    "        return old_labels  # skip lines like 'TOTAL $100'\n",
    "\n",
    "    # Index all numeric tokens (excluding those already labeled DESCRIPTION)\n",
    "    numeric_indices = [\n",
    "        i for i, t in enumerate(tokens_with_labels)\n",
    "        if is_numeric(t['text']) and old_labels[i] != \"DESCRIPTION\"\n",
    "    ]\n",
    "\n",
    "    # Initialize new label list with original labels\n",
    "    new_labels = old_labels[:]\n",
    "\n",
    "    # Assign PRICE to last numeric, UNIT_PRICE to second-last, QUANTITY to earliest\n",
    "   # Assign PRICE\n",
    "    if len(numeric_indices) >= 1:\n",
    "        new_labels[numeric_indices[-1]] = \"PRICE\"\n",
    "\n",
    "# Check middle value: if it's a small integer, it's likely QUANTITY not UNIT_PRICE\n",
    "    if len(numeric_indices) >= 2:\n",
    "        val = tokens_with_labels[numeric_indices[-2]][\"text\"]\n",
    "        val_clean = re.sub(r'[₹$]', '', val)\n",
    "        if val_clean.isdigit() and int(val_clean) <= 10:\n",
    "            new_labels[numeric_indices[-2]] = \"QUANTITY\"\n",
    "        else:\n",
    "            new_labels[numeric_indices[-2]] = \"UNIT_PRICE\"\n",
    "\n",
    "# Assign QUANTITY to earliest number (only if not already set)\n",
    "    if len(numeric_indices) >= 3:\n",
    "        if new_labels[numeric_indices[0]] not in (\"QUANTITY\", \"PRICE\"):\n",
    "            new_labels[numeric_indices[0]] = \"QUANTITY\"\n",
    "\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d7ad118-4705-4c9b-bcb1-7d086460a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your model and cleaned labels\n",
    "for r in filtered:\n",
    "    tokens = extract_lines_with_tokens(ip)[r['line_id'] - 1]['tokens']\n",
    "    token_list = []\n",
    "    for tok, label, score in zip(tokens, r[\"cleaned_preds\"], r[\"word_scores\"]):\n",
    "        token_list.append({\n",
    "            \"text\": tok[\"text\"],\n",
    "            \"label\": label,\n",
    "            \"score\": score\n",
    "        })\n",
    "    \n",
    "    corrected = apply_template_if_description(token_list)\n",
    "    r[\"final_labels\"] = corrected  # store it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "88a7ff04-6f13-4971-90d8-83281be3e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_food_format_table(filtered, img_path):\n",
    "    table = []\n",
    "    for r in filtered:\n",
    "        tokens = extract_lines_with_tokens(img_path)[r['line_id'] - 1]['tokens']\n",
    "        token_list = [\n",
    "            {\"text\": tok[\"text\"], \"label\": label}\n",
    "            for tok, label in zip(tokens, r[\"final_labels\"])\n",
    "        ]\n",
    "\n",
    "        # Group words by label\n",
    "        qty         = \" \".join(t[\"text\"] for t in token_list if t[\"label\"] == \"QUANTITY\")\n",
    "        description = \" \".join(t[\"text\"] for t in token_list if t[\"label\"] == \"DESCRIPTION\")\n",
    "        unit_price  = \" \".join(t[\"text\"] for t in token_list if t[\"label\"] == \"UNIT_PRICE\")\n",
    "        price       = \" \".join(t[\"text\"] for t in token_list if t[\"label\"] == \"PRICE\")\n",
    "\n",
    "        table.append([r[\"line_id\"], qty, description, unit_price, price])\n",
    "\n",
    "    headers = [\"Line\", \"Quantity\", \"Description\", \"Unit Price\", \"Price\"]\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf1d38e8-5525-413b-b1a0-57f34a3e6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------------------------+--------------+---------+\n",
      "|   Line | Quantity   | Description                 | Unit Price   | Price   |\n",
      "+========+============+=============================+==============+=========+\n",
      "|     12 | 1          | Front and rear brake cables | 100.00       | 100.00  |\n",
      "+--------+------------+-----------------------------+--------------+---------+\n",
      "|     13 | 2          | set of pedal arms           | 16.00        | 30.00   |\n",
      "+--------+------------+-----------------------------+--------------+---------+\n",
      "|     14 | 3          | Labor                       | 5.00         | 15.00   |\n",
      "+--------+------------+-----------------------------+--------------+---------+\n",
      "|     16 |            |                             |              |         |\n",
      "+--------+------------+-----------------------------+--------------+---------+\n",
      "|     17 |            |                             |              |         |\n",
      "+--------+------------+-----------------------------+--------------+---------+\n"
     ]
    }
   ],
   "source": [
    "print_food_format_table(filtered, ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc97c76-50af-46c8-bedb-8408127bec66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
